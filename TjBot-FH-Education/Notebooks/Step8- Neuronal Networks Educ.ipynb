{"nbformat_minor": 1, "cells": [{"source": "# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nfrom project_lib import Project\nproject = Project(project_id='6a2c2f45-a184-4ae6-a9d3-3a1e5ad4e537', project_access_token='p-d7bc36affb87b44b1b105d6290da63dfda68762b')\npc = project.project_context", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 2}, {"source": "import matplotlib\nfrom matplotlib import pylab as plt\nimport numpy as np\nimport pandas as pd", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 8}, {"source": "\nimport types\nimport pandas as pd\nfrom ibm_botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share your notebook.\nclient_acece927b4ba44519751d49fd8861145 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='as0X-g5so9ras2AQdxGGQ7-cHrl7uuAmjsmgdU3C0ebW',\n    ibm_auth_endpoint=\"https://iam.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.eu-geo.objectstorage.service.networklayer.com')\n\nbody = client_acece927b4ba44519751d49fd8861145.get_object(Bucket='tjbotai-donotdelete-pr-82k5ks6cxagfsz',Key='Wetter-Linz-Hoersching.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf = pd.read_csv(body, parse_dates=['DateTime [UTC]'])\ndf.head()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DateTime [UTC]</th>\n      <th>T</th>\n      <th>Td</th>\n      <th>ff</th>\n      <th>dd</th>\n      <th>P</th>\n      <th>Pstat</th>\n      <th>rF</th>\n      <th>rSD</th>\n      <th>RR</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2016-12-31 23:00:00</td>\n      <td>-5.1</td>\n      <td>-5.5</td>\n      <td>1.11</td>\n      <td>135.0</td>\n      <td>1032.6</td>\n      <td>NaN</td>\n      <td>97.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-01-01 00:00:00</td>\n      <td>-5.1</td>\n      <td>-5.5</td>\n      <td>0.56</td>\n      <td>NaN</td>\n      <td>1031.7</td>\n      <td>NaN</td>\n      <td>97.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-01-01 01:00:00</td>\n      <td>-5.2</td>\n      <td>-5.7</td>\n      <td>1.11</td>\n      <td>315.0</td>\n      <td>1031.1</td>\n      <td>NaN</td>\n      <td>97.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-01-01 02:00:00</td>\n      <td>-5.6</td>\n      <td>-6.1</td>\n      <td>1.11</td>\n      <td>315.0</td>\n      <td>1030.5</td>\n      <td>NaN</td>\n      <td>96.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2017-01-01 03:00:00</td>\n      <td>-5.8</td>\n      <td>-6.3</td>\n      <td>1.11</td>\n      <td>270.0</td>\n      <td>1029.8</td>\n      <td>NaN</td>\n      <td>96.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "       DateTime [UTC]    T   Td    ff     dd       P  Pstat    rF  rSD   RR\n0 2016-12-31 23:00:00 -5.1 -5.5  1.11  135.0  1032.6    NaN  97.0  0.0  0.0\n1 2017-01-01 00:00:00 -5.1 -5.5  0.56    NaN  1031.7    NaN  97.0  0.0  0.0\n2 2017-01-01 01:00:00 -5.2 -5.7  1.11  315.0  1031.1    NaN  97.0  0.0  0.0\n3 2017-01-01 02:00:00 -5.6 -6.1  1.11  315.0  1030.5    NaN  96.0  0.0  0.0\n4 2017-01-01 03:00:00 -5.8 -6.3  1.11  270.0  1029.8    NaN  96.0  0.0  0.0"}, "execution_count": 16, "metadata": {}}], "execution_count": 16}, {"source": "df = df.drop(columns=['Pstat']).dropna().set_index('DateTime [UTC]')\ndf", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>T</th>\n      <th>Td</th>\n      <th>ff</th>\n      <th>dd</th>\n      <th>P</th>\n      <th>rF</th>\n      <th>rSD</th>\n      <th>RR</th>\n    </tr>\n    <tr>\n      <th>DateTime [UTC]</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2016-12-31 23:00:00</th>\n      <td>-5.1</td>\n      <td>-5.5</td>\n      <td>1.110000</td>\n      <td>135.0</td>\n      <td>1032.6</td>\n      <td>97.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 01:00:00</th>\n      <td>-5.2</td>\n      <td>-5.7</td>\n      <td>1.110000</td>\n      <td>315.0</td>\n      <td>1031.1</td>\n      <td>97.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 02:00:00</th>\n      <td>-5.6</td>\n      <td>-6.1</td>\n      <td>1.110000</td>\n      <td>315.0</td>\n      <td>1030.5</td>\n      <td>96.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 03:00:00</th>\n      <td>-5.8</td>\n      <td>-6.3</td>\n      <td>1.110000</td>\n      <td>270.0</td>\n      <td>1029.8</td>\n      <td>96.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 05:00:00</th>\n      <td>-6.3</td>\n      <td>-6.8</td>\n      <td>1.110000</td>\n      <td>270.0</td>\n      <td>1028.9</td>\n      <td>96.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 06:00:00</th>\n      <td>-6.6</td>\n      <td>-7.2</td>\n      <td>1.110000</td>\n      <td>225.0</td>\n      <td>1028.5</td>\n      <td>95.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 07:00:00</th>\n      <td>-6.2</td>\n      <td>-6.8</td>\n      <td>1.390000</td>\n      <td>270.0</td>\n      <td>1028.4</td>\n      <td>96.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 08:00:00</th>\n      <td>-6.2</td>\n      <td>-6.7</td>\n      <td>1.110000</td>\n      <td>135.0</td>\n      <td>1028.1</td>\n      <td>96.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 09:00:00</th>\n      <td>-5.8</td>\n      <td>-6.3</td>\n      <td>1.110000</td>\n      <td>270.0</td>\n      <td>1027.7</td>\n      <td>96.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 12:00:00</th>\n      <td>-5.3</td>\n      <td>-5.7</td>\n      <td>1.110000</td>\n      <td>225.0</td>\n      <td>1025.0</td>\n      <td>97.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 13:00:00</th>\n      <td>-5.1</td>\n      <td>-5.7</td>\n      <td>1.110000</td>\n      <td>315.0</td>\n      <td>1024.3</td>\n      <td>96.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 14:00:00</th>\n      <td>-4.9</td>\n      <td>-5.4</td>\n      <td>1.390000</td>\n      <td>225.0</td>\n      <td>1023.9</td>\n      <td>96.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 15:00:00</th>\n      <td>-4.9</td>\n      <td>-5.4</td>\n      <td>1.110000</td>\n      <td>225.0</td>\n      <td>1023.7</td>\n      <td>96.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 16:00:00</th>\n      <td>-4.9</td>\n      <td>-5.4</td>\n      <td>1.110000</td>\n      <td>270.0</td>\n      <td>1023.4</td>\n      <td>96.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 17:00:00</th>\n      <td>-5.3</td>\n      <td>-5.8</td>\n      <td>1.110000</td>\n      <td>270.0</td>\n      <td>1023.3</td>\n      <td>96.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 18:00:00</th>\n      <td>-5.7</td>\n      <td>-6.3</td>\n      <td>1.390000</td>\n      <td>225.0</td>\n      <td>1023.6</td>\n      <td>96.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 19:00:00</th>\n      <td>-6.0</td>\n      <td>-6.5</td>\n      <td>1.110000</td>\n      <td>225.0</td>\n      <td>1023.5</td>\n      <td>96.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 20:00:00</th>\n      <td>-5.8</td>\n      <td>-6.4</td>\n      <td>1.110000</td>\n      <td>90.0</td>\n      <td>1023.4</td>\n      <td>96.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 22:00:00</th>\n      <td>-6.5</td>\n      <td>-7.2</td>\n      <td>1.110000</td>\n      <td>315.0</td>\n      <td>1023.2</td>\n      <td>95.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-01 23:00:00</th>\n      <td>-6.6</td>\n      <td>-7.4</td>\n      <td>1.390000</td>\n      <td>225.0</td>\n      <td>1023.2</td>\n      <td>94.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-02 00:00:00</th>\n      <td>-6.9</td>\n      <td>-7.7</td>\n      <td>1.390000</td>\n      <td>225.0</td>\n      <td>1023.0</td>\n      <td>94.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-02 01:00:00</th>\n      <td>-6.5</td>\n      <td>-7.2</td>\n      <td>1.390000</td>\n      <td>225.0</td>\n      <td>1022.8</td>\n      <td>95.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-02 02:00:00</th>\n      <td>-6.1</td>\n      <td>-6.8</td>\n      <td>2.220000</td>\n      <td>270.0</td>\n      <td>1022.6</td>\n      <td>95.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-02 03:00:00</th>\n      <td>-7.1</td>\n      <td>-7.8</td>\n      <td>1.390000</td>\n      <td>270.0</td>\n      <td>1022.7</td>\n      <td>95.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-02 04:00:00</th>\n      <td>-7.6</td>\n      <td>-8.4</td>\n      <td>1.110000</td>\n      <td>270.0</td>\n      <td>1022.6</td>\n      <td>94.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-02 05:00:00</th>\n      <td>-8.1</td>\n      <td>-9.2</td>\n      <td>1.390000</td>\n      <td>225.0</td>\n      <td>1022.7</td>\n      <td>92.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-02 06:00:00</th>\n      <td>-8.6</td>\n      <td>-9.7</td>\n      <td>0.560000</td>\n      <td>270.0</td>\n      <td>1023.0</td>\n      <td>92.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-02 08:00:00</th>\n      <td>-5.8</td>\n      <td>-6.4</td>\n      <td>4.170000</td>\n      <td>270.0</td>\n      <td>1022.6</td>\n      <td>96.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-02 09:00:00</th>\n      <td>-5.2</td>\n      <td>-5.9</td>\n      <td>2.220000</td>\n      <td>315.0</td>\n      <td>1022.6</td>\n      <td>95.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2017-01-02 10:00:00</th>\n      <td>-3.6</td>\n      <td>-4.1</td>\n      <td>6.670000</td>\n      <td>270.0</td>\n      <td>1022.0</td>\n      <td>96.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2018-11-09 08:00:00</th>\n      <td>10.2</td>\n      <td>7.3</td>\n      <td>2.222222</td>\n      <td>90.0</td>\n      <td>1019.0</td>\n      <td>82.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-09 09:00:00</th>\n      <td>10.4</td>\n      <td>7.5</td>\n      <td>2.500000</td>\n      <td>90.0</td>\n      <td>1018.7</td>\n      <td>82.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-09 10:00:00</th>\n      <td>10.6</td>\n      <td>7.5</td>\n      <td>2.500000</td>\n      <td>90.0</td>\n      <td>1018.8</td>\n      <td>81.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-09 11:00:00</th>\n      <td>10.7</td>\n      <td>7.6</td>\n      <td>1.111111</td>\n      <td>90.0</td>\n      <td>1018.5</td>\n      <td>82.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-09 12:00:00</th>\n      <td>11.1</td>\n      <td>7.3</td>\n      <td>1.388889</td>\n      <td>90.0</td>\n      <td>1018.0</td>\n      <td>78.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-09 13:00:00</th>\n      <td>11.0</td>\n      <td>7.3</td>\n      <td>1.111111</td>\n      <td>45.0</td>\n      <td>1017.6</td>\n      <td>78.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-09 14:00:00</th>\n      <td>10.9</td>\n      <td>7.3</td>\n      <td>1.388889</td>\n      <td>90.0</td>\n      <td>1017.2</td>\n      <td>78.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-09 15:00:00</th>\n      <td>10.6</td>\n      <td>7.4</td>\n      <td>2.222222</td>\n      <td>90.0</td>\n      <td>1016.8</td>\n      <td>81.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-09 16:00:00</th>\n      <td>10.3</td>\n      <td>7.5</td>\n      <td>2.500000</td>\n      <td>90.0</td>\n      <td>1016.9</td>\n      <td>83.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-09 17:00:00</th>\n      <td>10.3</td>\n      <td>7.4</td>\n      <td>1.388889</td>\n      <td>90.0</td>\n      <td>1017.1</td>\n      <td>83.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-09 18:00:00</th>\n      <td>10.3</td>\n      <td>7.5</td>\n      <td>2.500000</td>\n      <td>90.0</td>\n      <td>1016.9</td>\n      <td>83.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-09 19:00:00</th>\n      <td>10.2</td>\n      <td>7.5</td>\n      <td>2.500000</td>\n      <td>90.0</td>\n      <td>1016.7</td>\n      <td>83.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-09 20:00:00</th>\n      <td>9.9</td>\n      <td>7.4</td>\n      <td>2.500000</td>\n      <td>90.0</td>\n      <td>1016.6</td>\n      <td>84.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-09 21:00:00</th>\n      <td>9.8</td>\n      <td>7.6</td>\n      <td>2.500000</td>\n      <td>90.0</td>\n      <td>1016.3</td>\n      <td>86.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-09 22:00:00</th>\n      <td>9.7</td>\n      <td>7.6</td>\n      <td>3.055556</td>\n      <td>90.0</td>\n      <td>1016.1</td>\n      <td>87.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-09 23:00:00</th>\n      <td>9.7</td>\n      <td>7.7</td>\n      <td>2.500000</td>\n      <td>90.0</td>\n      <td>1015.6</td>\n      <td>88.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-10 01:00:00</th>\n      <td>9.5</td>\n      <td>8.0</td>\n      <td>1.388889</td>\n      <td>90.0</td>\n      <td>1015.0</td>\n      <td>90.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-10 02:00:00</th>\n      <td>9.4</td>\n      <td>8.2</td>\n      <td>1.388889</td>\n      <td>90.0</td>\n      <td>1014.6</td>\n      <td>93.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-10 03:00:00</th>\n      <td>9.3</td>\n      <td>8.2</td>\n      <td>2.222222</td>\n      <td>90.0</td>\n      <td>1014.3</td>\n      <td>92.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-10 04:00:00</th>\n      <td>9.3</td>\n      <td>7.6</td>\n      <td>2.222222</td>\n      <td>90.0</td>\n      <td>1014.1</td>\n      <td>89.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-10 05:00:00</th>\n      <td>9.0</td>\n      <td>7.3</td>\n      <td>1.388889</td>\n      <td>45.0</td>\n      <td>1013.9</td>\n      <td>89.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-10 06:00:00</th>\n      <td>9.0</td>\n      <td>7.4</td>\n      <td>1.388889</td>\n      <td>90.0</td>\n      <td>1013.9</td>\n      <td>90.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-10 07:00:00</th>\n      <td>9.0</td>\n      <td>7.3</td>\n      <td>1.388889</td>\n      <td>90.0</td>\n      <td>1014.1</td>\n      <td>89.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-10 08:00:00</th>\n      <td>9.4</td>\n      <td>7.0</td>\n      <td>2.500000</td>\n      <td>90.0</td>\n      <td>1014.2</td>\n      <td>85.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-10 09:00:00</th>\n      <td>9.7</td>\n      <td>7.1</td>\n      <td>2.222222</td>\n      <td>90.0</td>\n      <td>1013.9</td>\n      <td>84.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-10 10:00:00</th>\n      <td>10.0</td>\n      <td>6.9</td>\n      <td>3.611111</td>\n      <td>90.0</td>\n      <td>1013.7</td>\n      <td>81.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-10 11:00:00</th>\n      <td>10.3</td>\n      <td>7.0</td>\n      <td>3.611111</td>\n      <td>90.0</td>\n      <td>1013.1</td>\n      <td>80.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-10 12:00:00</th>\n      <td>10.5</td>\n      <td>7.3</td>\n      <td>3.055556</td>\n      <td>90.0</td>\n      <td>1012.8</td>\n      <td>80.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-10 13:00:00</th>\n      <td>10.6</td>\n      <td>7.2</td>\n      <td>2.500000</td>\n      <td>90.0</td>\n      <td>1012.4</td>\n      <td>79.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-11-10 14:00:00</th>\n      <td>10.1</td>\n      <td>7.1</td>\n      <td>4.166667</td>\n      <td>90.0</td>\n      <td>1012.2</td>\n      <td>82.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>14789 rows \u00d7 8 columns</p>\n</div>", "text/plain": "                        T   Td        ff     dd       P    rF  rSD   RR\nDateTime [UTC]                                                         \n2016-12-31 23:00:00  -5.1 -5.5  1.110000  135.0  1032.6  97.0  0.0  0.0\n2017-01-01 01:00:00  -5.2 -5.7  1.110000  315.0  1031.1  97.0  0.0  0.0\n2017-01-01 02:00:00  -5.6 -6.1  1.110000  315.0  1030.5  96.0  0.0  0.0\n2017-01-01 03:00:00  -5.8 -6.3  1.110000  270.0  1029.8  96.0  0.0  0.0\n2017-01-01 05:00:00  -6.3 -6.8  1.110000  270.0  1028.9  96.0  0.0  0.0\n2017-01-01 06:00:00  -6.6 -7.2  1.110000  225.0  1028.5  95.0  0.0  0.0\n2017-01-01 07:00:00  -6.2 -6.8  1.390000  270.0  1028.4  96.0  0.0  0.0\n2017-01-01 08:00:00  -6.2 -6.7  1.110000  135.0  1028.1  96.0  0.0  0.0\n2017-01-01 09:00:00  -5.8 -6.3  1.110000  270.0  1027.7  96.0  0.0  0.0\n2017-01-01 12:00:00  -5.3 -5.7  1.110000  225.0  1025.0  97.0  0.0  0.0\n2017-01-01 13:00:00  -5.1 -5.7  1.110000  315.0  1024.3  96.0  0.0  0.0\n2017-01-01 14:00:00  -4.9 -5.4  1.390000  225.0  1023.9  96.0  0.0  0.0\n2017-01-01 15:00:00  -4.9 -5.4  1.110000  225.0  1023.7  96.0  0.0  0.0\n2017-01-01 16:00:00  -4.9 -5.4  1.110000  270.0  1023.4  96.0  0.0  0.0\n2017-01-01 17:00:00  -5.3 -5.8  1.110000  270.0  1023.3  96.0  0.0  0.0\n2017-01-01 18:00:00  -5.7 -6.3  1.390000  225.0  1023.6  96.0  0.0  0.0\n2017-01-01 19:00:00  -6.0 -6.5  1.110000  225.0  1023.5  96.0  0.0  0.0\n2017-01-01 20:00:00  -5.8 -6.4  1.110000   90.0  1023.4  96.0  0.0  0.0\n2017-01-01 22:00:00  -6.5 -7.2  1.110000  315.0  1023.2  95.0  0.0  0.0\n2017-01-01 23:00:00  -6.6 -7.4  1.390000  225.0  1023.2  94.0  0.0  0.0\n2017-01-02 00:00:00  -6.9 -7.7  1.390000  225.0  1023.0  94.0  0.0  0.0\n2017-01-02 01:00:00  -6.5 -7.2  1.390000  225.0  1022.8  95.0  0.0  0.0\n2017-01-02 02:00:00  -6.1 -6.8  2.220000  270.0  1022.6  95.0  0.0  0.0\n2017-01-02 03:00:00  -7.1 -7.8  1.390000  270.0  1022.7  95.0  0.0  0.0\n2017-01-02 04:00:00  -7.6 -8.4  1.110000  270.0  1022.6  94.0  0.0  0.0\n2017-01-02 05:00:00  -8.1 -9.2  1.390000  225.0  1022.7  92.0  0.0  0.0\n2017-01-02 06:00:00  -8.6 -9.7  0.560000  270.0  1023.0  92.0  0.0  0.0\n2017-01-02 08:00:00  -5.8 -6.4  4.170000  270.0  1022.6  96.0  0.0  0.0\n2017-01-02 09:00:00  -5.2 -5.9  2.220000  315.0  1022.6  95.0  0.0  0.0\n2017-01-02 10:00:00  -3.6 -4.1  6.670000  270.0  1022.0  96.0  0.0  0.0\n...                   ...  ...       ...    ...     ...   ...  ...  ...\n2018-11-09 08:00:00  10.2  7.3  2.222222   90.0  1019.0  82.0  0.0  0.0\n2018-11-09 09:00:00  10.4  7.5  2.500000   90.0  1018.7  82.0  0.0  0.0\n2018-11-09 10:00:00  10.6  7.5  2.500000   90.0  1018.8  81.0  0.0  0.0\n2018-11-09 11:00:00  10.7  7.6  1.111111   90.0  1018.5  82.0  0.0  0.0\n2018-11-09 12:00:00  11.1  7.3  1.388889   90.0  1018.0  78.0  0.0  0.0\n2018-11-09 13:00:00  11.0  7.3  1.111111   45.0  1017.6  78.0  0.0  0.0\n2018-11-09 14:00:00  10.9  7.3  1.388889   90.0  1017.2  78.0  0.0  0.0\n2018-11-09 15:00:00  10.6  7.4  2.222222   90.0  1016.8  81.0  0.0  0.0\n2018-11-09 16:00:00  10.3  7.5  2.500000   90.0  1016.9  83.0  0.0  0.0\n2018-11-09 17:00:00  10.3  7.4  1.388889   90.0  1017.1  83.0  0.0  0.0\n2018-11-09 18:00:00  10.3  7.5  2.500000   90.0  1016.9  83.0  0.0  0.0\n2018-11-09 19:00:00  10.2  7.5  2.500000   90.0  1016.7  83.0  0.0  0.0\n2018-11-09 20:00:00   9.9  7.4  2.500000   90.0  1016.6  84.0  0.0  0.0\n2018-11-09 21:00:00   9.8  7.6  2.500000   90.0  1016.3  86.0  0.0  0.0\n2018-11-09 22:00:00   9.7  7.6  3.055556   90.0  1016.1  87.0  0.0  0.0\n2018-11-09 23:00:00   9.7  7.7  2.500000   90.0  1015.6  88.0  0.0  0.0\n2018-11-10 01:00:00   9.5  8.0  1.388889   90.0  1015.0  90.0  0.0  0.0\n2018-11-10 02:00:00   9.4  8.2  1.388889   90.0  1014.6  93.0  0.0  0.0\n2018-11-10 03:00:00   9.3  8.2  2.222222   90.0  1014.3  92.0  0.0  0.0\n2018-11-10 04:00:00   9.3  7.6  2.222222   90.0  1014.1  89.0  0.0  0.0\n2018-11-10 05:00:00   9.0  7.3  1.388889   45.0  1013.9  89.0  0.0  0.0\n2018-11-10 06:00:00   9.0  7.4  1.388889   90.0  1013.9  90.0  0.0  0.0\n2018-11-10 07:00:00   9.0  7.3  1.388889   90.0  1014.1  89.0  0.0  0.0\n2018-11-10 08:00:00   9.4  7.0  2.500000   90.0  1014.2  85.0  0.0  0.0\n2018-11-10 09:00:00   9.7  7.1  2.222222   90.0  1013.9  84.0  0.0  0.0\n2018-11-10 10:00:00  10.0  6.9  3.611111   90.0  1013.7  81.0  0.0  0.0\n2018-11-10 11:00:00  10.3  7.0  3.611111   90.0  1013.1  80.0  0.0  0.0\n2018-11-10 12:00:00  10.5  7.3  3.055556   90.0  1012.8  80.0  0.0  0.0\n2018-11-10 13:00:00  10.6  7.2  2.500000   90.0  1012.4  79.0  0.0  0.0\n2018-11-10 14:00:00  10.1  7.1  4.166667   90.0  1012.2  82.0  0.0  0.0\n\n[14789 rows x 8 columns]"}, "execution_count": 17, "metadata": {}}], "execution_count": 17}, {"source": "data = df.iloc[0:6000:2]", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 18}, {"source": "X = data[['T','P','rF']]\nY = data['Td']", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 19}, {"source": "from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_predict, KFold\nfrom sklearn.metrics import r2_score, mean_squared_error", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 24}, {"source": "lr = LinearRegression()\ny_pred_lr = cross_val_predict(lr, X, Y, cv=KFold(3, random_state=1))\nlr.fit(X,Y)\nr2_score(Y, y_pred_lr), np.sqrt(mean_squared_error(Y, y_pred_lr)), np.max(np.abs(Y-y_pred_lr))", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "(0.98362928752298751, 0.96412532092270919, 9.47581544865651)"}, "execution_count": 26, "metadata": {}}], "execution_count": 26}, {"source": "from sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 28}, {"source": "nn = Pipeline([\n    ('scale', StandardScaler()),\n    ('mlp', MLPRegressor(\n        hidden_layer_sizes = [5,5,5], \n        max_iter = 1000\n    ))\n])", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 35}, {"source": "nn.fit(X,Y)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "Pipeline(memory=None,\n     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('mlp', MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n       hidden_layer_sizes=[5, 5, 5], learning_rate='constant',\n       learning_ra...=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n       verbose=False, warm_start=False))])"}, "execution_count": 36, "metadata": {}}], "execution_count": 36}, {"source": "y_pred_nn = cross_val_predict(nn, X, Y, cv=KFold(3, random_state=1))\nnn.fit(X,Y)\nr2_score(Y, y_pred_nn), np.sqrt(mean_squared_error(Y, y_pred_nn)), np.max(np.abs(Y-y_pred_nn))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "from sklearn.preprocessing import PolynomialFeatures", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 37}, {"source": "pr=Pipeline([\n    ('ploy', PolynomialFeatures(degree=4)),\n    ('lr', LinearRegression())\n])", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 42}, {"source": "y_pred_pr = cross_val_predict(pr, X, Y, cv=KFold(3, random_state=1))\nr2_score(Y, y_pred_pr), np.sqrt(mean_squared_error(Y, y_pred_pr)), np.max(np.abs(Y-y_pred_pr))", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "(0.9996595820584836, 0.1390291664889492, 3.9706142711598655)"}, "execution_count": 43, "metadata": {}}], "execution_count": 43}, {"source": "# MNIST Data", "cell_type": "markdown", "metadata": {}}, {"source": "from sklearn import datasets\niris = datasets.load_iris()\ndigits = datasets.load_digits()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  return f(*args, **kwds)\n/opt/conda/envs/DSX-Python35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  return f(*args, **kwds)\n/opt/conda/envs/DSX-Python35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  return f(*args, **kwds)\n/opt/conda/envs/DSX-Python35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  return f(*args, **kwds)\n"}], "execution_count": 1}, {"source": "", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "# Deep Learning with Keras", "cell_type": "markdown", "metadata": {}}, {"source": "import keras\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 2}, {"source": "from sklearn.model_selection import train_test_split", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 3}, {"source": "X_train, X_test, y_train_classes, y_test_classes = train_test_split(digits.data, digits.target, test_size=0.33, random_state=1)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 5}, {"source": "X_train.shape", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "(1203, 64)"}, "execution_count": 6, "metadata": {}}], "execution_count": 6}, {"source": "X_test.shape", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "(594, 64)"}, "execution_count": 8, "metadata": {}}], "execution_count": 8}, {"source": "y_train_classes", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([1, 9, 0, ..., 9, 1, 5])"}, "execution_count": 9, "metadata": {}}], "execution_count": 9}, {"source": "# OneHot coding \ny_train = to_categorical(y_train_classes)\ny_test = to_categorical(y_test_classes)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 14}, {"source": "y_train.shape", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "(1203, 10)"}, "execution_count": 11, "metadata": {}}], "execution_count": 11}, {"source": "y_train[1]", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.])"}, "execution_count": 12, "metadata": {}}], "execution_count": 12}, {"source": "mlp = Sequential()\nmlp.add( Dense(128, activation='relu', input_shape=(64,)) )\nmlp.add( Dense(128, activation='relu'))\nmlp.add( Dense(10, activation='softmax') )\nmlp.compile(\n    loss= keras.losses.categorical_crossentropy,\n    optimizer = keras.optimizers.Adam(),\n    metrics = ['accuracy']\n)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 15}, {"source": "mlp.fit(\n    X_train, y_train,\n    batch_size = 64,\n    epochs = 50,\n    verbose = 1, \n    validation_data=(X_test, y_test)\n)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Train on 1203 samples, validate on 594 samples\nEpoch 1/50\n1203/1203 [==============================] - 0s 145us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.0676 - val_acc: 0.9747\nEpoch 2/50\n1203/1203 [==============================] - 0s 91us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0707 - val_acc: 0.9731\nEpoch 3/50\n1203/1203 [==============================] - 0s 151us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.0667 - val_acc: 0.9747\nEpoch 4/50\n1203/1203 [==============================] - 0s 95us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0661 - val_acc: 0.9747\nEpoch 5/50\n1203/1203 [==============================] - 0s 148us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.0672 - val_acc: 0.9747\nEpoch 6/50\n1203/1203 [==============================] - 0s 95us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0644 - val_acc: 0.9747\nEpoch 7/50\n1203/1203 [==============================] - 0s 96us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0644 - val_acc: 0.9747\nEpoch 8/50\n1203/1203 [==============================] - 0s 146us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0651 - val_acc: 0.9747\nEpoch 9/50\n1203/1203 [==============================] - 0s 94us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0642 - val_acc: 0.9747\nEpoch 10/50\n1203/1203 [==============================] - 0s 148us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0640 - val_acc: 0.9764\nEpoch 11/50\n1203/1203 [==============================] - 0s 96us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0643 - val_acc: 0.9747\nEpoch 12/50\n1203/1203 [==============================] - 0s 152us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9731\nEpoch 13/50\n1203/1203 [==============================] - 0s 94us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0639 - val_acc: 0.9764\nEpoch 14/50\n1203/1203 [==============================] - 0s 91us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0638 - val_acc: 0.9747\nEpoch 15/50\n1203/1203 [==============================] - 0s 150us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9731\nEpoch 16/50\n1203/1203 [==============================] - 0s 94us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0637 - val_acc: 0.9764\nEpoch 17/50\n1203/1203 [==============================] - 0s 150us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0641 - val_acc: 0.9747\nEpoch 18/50\n1203/1203 [==============================] - 0s 93us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0634 - val_acc: 0.9781\nEpoch 19/50\n1203/1203 [==============================] - 0s 93us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0650 - val_acc: 0.9747\nEpoch 20/50\n1203/1203 [==============================] - 0s 151us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0643 - val_acc: 0.9764\nEpoch 21/50\n1203/1203 [==============================] - 0s 94us/step - loss: 9.8311e-04 - acc: 1.0000 - val_loss: 0.0641 - val_acc: 0.9747\nEpoch 22/50\n1203/1203 [==============================] - 0s 148us/step - loss: 9.4529e-04 - acc: 1.0000 - val_loss: 0.0642 - val_acc: 0.9764\nEpoch 23/50\n1203/1203 [==============================] - 0s 94us/step - loss: 9.1552e-04 - acc: 1.0000 - val_loss: 0.0640 - val_acc: 0.9747\nEpoch 24/50\n1203/1203 [==============================] - 0s 94us/step - loss: 8.4638e-04 - acc: 1.0000 - val_loss: 0.0653 - val_acc: 0.9731\nEpoch 25/50\n1203/1203 [==============================] - 0s 147us/step - loss: 8.0802e-04 - acc: 1.0000 - val_loss: 0.0653 - val_acc: 0.9731\nEpoch 26/50\n1203/1203 [==============================] - 0s 92us/step - loss: 7.4928e-04 - acc: 1.0000 - val_loss: 0.0645 - val_acc: 0.9747\nEpoch 27/50\n1203/1203 [==============================] - 0s 148us/step - loss: 7.0400e-04 - acc: 1.0000 - val_loss: 0.0654 - val_acc: 0.9731\nEpoch 28/50\n1203/1203 [==============================] - 0s 92us/step - loss: 6.7747e-04 - acc: 1.0000 - val_loss: 0.0651 - val_acc: 0.9747\nEpoch 29/50\n1203/1203 [==============================] - 0s 92us/step - loss: 6.3905e-04 - acc: 1.0000 - val_loss: 0.0650 - val_acc: 0.9747\nEpoch 30/50\n1203/1203 [==============================] - 0s 150us/step - loss: 6.1526e-04 - acc: 1.0000 - val_loss: 0.0651 - val_acc: 0.9747\nEpoch 31/50\n1203/1203 [==============================] - 0s 92us/step - loss: 5.9342e-04 - acc: 1.0000 - val_loss: 0.0646 - val_acc: 0.9781\nEpoch 32/50\n1203/1203 [==============================] - 0s 146us/step - loss: 5.6483e-04 - acc: 1.0000 - val_loss: 0.0656 - val_acc: 0.9747\nEpoch 33/50\n1203/1203 [==============================] - 0s 96us/step - loss: 5.3725e-04 - acc: 1.0000 - val_loss: 0.0652 - val_acc: 0.9747\nEpoch 34/50\n1203/1203 [==============================] - 0s 93us/step - loss: 5.1100e-04 - acc: 1.0000 - val_loss: 0.0652 - val_acc: 0.9747\nEpoch 35/50\n1203/1203 [==============================] - 0s 150us/step - loss: 4.9013e-04 - acc: 1.0000 - val_loss: 0.0660 - val_acc: 0.9747\nEpoch 36/50\n1203/1203 [==============================] - 0s 93us/step - loss: 4.6823e-04 - acc: 1.0000 - val_loss: 0.0659 - val_acc: 0.9747\nEpoch 37/50\n1203/1203 [==============================] - 0s 92us/step - loss: 4.4781e-04 - acc: 1.0000 - val_loss: 0.0656 - val_acc: 0.9747\nEpoch 38/50\n1203/1203 [==============================] - 0s 148us/step - loss: 4.3536e-04 - acc: 1.0000 - val_loss: 0.0659 - val_acc: 0.9747\nEpoch 39/50\n1203/1203 [==============================] - 0s 92us/step - loss: 4.1478e-04 - acc: 1.0000 - val_loss: 0.0664 - val_acc: 0.9747\nEpoch 40/50\n1203/1203 [==============================] - 0s 149us/step - loss: 3.9757e-04 - acc: 1.0000 - val_loss: 0.0663 - val_acc: 0.9747\nEpoch 41/50\n1203/1203 [==============================] - 0s 93us/step - loss: 3.8609e-04 - acc: 1.0000 - val_loss: 0.0662 - val_acc: 0.9747\nEpoch 42/50\n1203/1203 [==============================] - 0s 95us/step - loss: 3.7415e-04 - acc: 1.0000 - val_loss: 0.0662 - val_acc: 0.9747\nEpoch 43/50\n1203/1203 [==============================] - 0s 150us/step - loss: 3.5953e-04 - acc: 1.0000 - val_loss: 0.0666 - val_acc: 0.9747\nEpoch 44/50\n1203/1203 [==============================] - 0s 93us/step - loss: 3.4472e-04 - acc: 1.0000 - val_loss: 0.0665 - val_acc: 0.9747\nEpoch 45/50\n1203/1203 [==============================] - 0s 152us/step - loss: 3.3549e-04 - acc: 1.0000 - val_loss: 0.0672 - val_acc: 0.9747\nEpoch 46/50\n1203/1203 [==============================] - 0s 92us/step - loss: 3.2030e-04 - acc: 1.0000 - val_loss: 0.0669 - val_acc: 0.9747\nEpoch 47/50\n1203/1203 [==============================] - 0s 147us/step - loss: 3.0871e-04 - acc: 1.0000 - val_loss: 0.0668 - val_acc: 0.9747\nEpoch 48/50\n1203/1203 [==============================] - 0s 95us/step - loss: 2.9800e-04 - acc: 1.0000 - val_loss: 0.0677 - val_acc: 0.9747\nEpoch 49/50\n1203/1203 [==============================] - 0s 94us/step - loss: 2.9030e-04 - acc: 1.0000 - val_loss: 0.0673 - val_acc: 0.9747\nEpoch 50/50\n1203/1203 [==============================] - 0s 149us/step - loss: 2.7884e-04 - acc: 1.0000 - val_loss: 0.0675 - val_acc: 0.9747\n"}, {"output_type": "execute_result", "data": {"text/plain": "<keras.callbacks.History at 0x7f37fc443cf8>"}, "execution_count": 21, "metadata": {}}], "execution_count": 21}, {"source": "mlp.predict(X_test[:2])", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([[  1.08965399e-10,   9.98627901e-01,   1.57720478e-06,\n          2.73402652e-08,   4.60455368e-07,   2.87192819e-07,\n          3.21270841e-06,   3.87887638e-07,   1.36629597e-03,\n          9.60793556e-10],\n       [  2.82334639e-07,   1.70471779e-07,   4.05736864e-05,\n          1.58408842e-08,   1.69222663e-08,   9.93413508e-01,\n          2.79791852e-08,   6.14842866e-03,   3.97056632e-04,\n          9.82825110e-09]], dtype=float32)"}, "execution_count": 22, "metadata": {}}], "execution_count": 22}, {"source": "mlp.predict_classes(X_test[:2])", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([1, 5])"}, "execution_count": 23, "metadata": {}}], "execution_count": 23}, {"source": "from keras.layers import Conv2D, Flatten, Dropout, BatchNormalization, MaxPooling2D", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 36}, {"source": "X_train_cnn = X_train.reshape(X_train.shape[0],8,8,1)\nX_test_cnn = X_test.reshape(X_test.shape[0],8,8,1)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 30}, {"source": "X_train_cnn.shape", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "(1203, 8, 8, 1)"}, "execution_count": 31, "metadata": {}}], "execution_count": 31}, {"source": "cnn = Sequential()\n\ncnn.add( Conv2D(filters=32, kernel_size=(2,2), strides=(1,1), activation='relu', input_shape = (8,8,1)))\n\ncnn.add( Conv2D(filters=32, kernel_size=(2,2), strides=(1,1), activation='relu'))\n\ncnn.add ( MaxPooling2D(pool_size =(2,2)))\n\ncnn.add ( Dropout(0.25))\n\ncnn.add ( Flatten() )\n\ncnn.add( Dense(128, activation='relu'))\n\ncnn.add( BatchNormalization() )\n\ncnn.add( Dense(10, activation='softmax') )\n\ncnn.compile(\n    loss= keras.losses.categorical_crossentropy,\n    optimizer = keras.optimizers.Adam(),\n    metrics = ['accuracy']\n)\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 37}, {"source": "cnn.fit(\n    X_train_cnn, y_train,\n    batch_size = 64,\n    epochs = 50,\n    verbose = 1, \n    validation_data=(X_test_cnn, y_test)\n)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Train on 1203 samples, validate on 594 samples\nEpoch 1/50\n1203/1203 [==============================] - 1s 772us/step - loss: 1.9569 - acc: 0.3566 - val_loss: 0.6319 - val_acc: 0.8687\nEpoch 2/50\n1203/1203 [==============================] - 1s 651us/step - loss: 0.7560 - acc: 0.7706 - val_loss: 0.3304 - val_acc: 0.9125\nEpoch 3/50\n1203/1203 [==============================] - 1s 586us/step - loss: 0.3786 - acc: 0.8936 - val_loss: 0.2511 - val_acc: 0.9360\nEpoch 4/50\n1203/1203 [==============================] - 1s 586us/step - loss: 0.2776 - acc: 0.9227 - val_loss: 0.1898 - val_acc: 0.9495\nEpoch 5/50\n1203/1203 [==============================] - 1s 584us/step - loss: 0.1806 - acc: 0.9568 - val_loss: 0.1562 - val_acc: 0.9630\nEpoch 6/50\n1203/1203 [==============================] - 1s 653us/step - loss: 0.1620 - acc: 0.9643 - val_loss: 0.1105 - val_acc: 0.9815\nEpoch 7/50\n1203/1203 [==============================] - 1s 588us/step - loss: 0.1269 - acc: 0.9726 - val_loss: 0.1162 - val_acc: 0.9781\nEpoch 8/50\n1203/1203 [==============================] - 1s 591us/step - loss: 0.1110 - acc: 0.9751 - val_loss: 0.0910 - val_acc: 0.9848\nEpoch 9/50\n1203/1203 [==============================] - 1s 646us/step - loss: 0.0915 - acc: 0.9734 - val_loss: 0.0742 - val_acc: 0.9865\nEpoch 10/50\n1203/1203 [==============================] - 1s 589us/step - loss: 0.0775 - acc: 0.9817 - val_loss: 0.0708 - val_acc: 0.9916\nEpoch 11/50\n1203/1203 [==============================] - 1s 585us/step - loss: 0.0638 - acc: 0.9925 - val_loss: 0.0665 - val_acc: 0.9933\nEpoch 12/50\n1203/1203 [==============================] - 1s 581us/step - loss: 0.0576 - acc: 0.9875 - val_loss: 0.0553 - val_acc: 0.9949\nEpoch 13/50\n1203/1203 [==============================] - 1s 584us/step - loss: 0.0522 - acc: 0.9909 - val_loss: 0.0510 - val_acc: 0.9933\nEpoch 14/50\n1203/1203 [==============================] - 1s 651us/step - loss: 0.0490 - acc: 0.9917 - val_loss: 0.0435 - val_acc: 0.9949\nEpoch 15/50\n1203/1203 [==============================] - 1s 583us/step - loss: 0.0428 - acc: 0.9933 - val_loss: 0.0447 - val_acc: 0.9949\nEpoch 16/50\n1203/1203 [==============================] - 1s 585us/step - loss: 0.0469 - acc: 0.9909 - val_loss: 0.0464 - val_acc: 0.9933\nEpoch 17/50\n1203/1203 [==============================] - 1s 585us/step - loss: 0.0410 - acc: 0.9900 - val_loss: 0.0419 - val_acc: 0.9933\nEpoch 18/50\n1203/1203 [==============================] - 1s 588us/step - loss: 0.0350 - acc: 0.9958 - val_loss: 0.0406 - val_acc: 0.9966\nEpoch 19/50\n1203/1203 [==============================] - 1s 646us/step - loss: 0.0287 - acc: 0.9958 - val_loss: 0.0342 - val_acc: 0.9966\nEpoch 20/50\n1203/1203 [==============================] - 1s 585us/step - loss: 0.0329 - acc: 0.9917 - val_loss: 0.0388 - val_acc: 0.9916\nEpoch 21/50\n1203/1203 [==============================] - 1s 577us/step - loss: 0.0282 - acc: 0.9975 - val_loss: 0.0325 - val_acc: 0.9966\nEpoch 22/50\n1203/1203 [==============================] - 1s 512us/step - loss: 0.0234 - acc: 0.9975 - val_loss: 0.0326 - val_acc: 0.9966\nEpoch 23/50\n1203/1203 [==============================] - 1s 583us/step - loss: 0.0244 - acc: 0.9958 - val_loss: 0.0324 - val_acc: 0.9933\nEpoch 24/50\n1203/1203 [==============================] - 1s 580us/step - loss: 0.0185 - acc: 0.9983 - val_loss: 0.0279 - val_acc: 0.9949\nEpoch 25/50\n1203/1203 [==============================] - 1s 584us/step - loss: 0.0192 - acc: 0.9958 - val_loss: 0.0285 - val_acc: 0.9966\nEpoch 26/50\n1203/1203 [==============================] - 1s 578us/step - loss: 0.0292 - acc: 0.9950 - val_loss: 0.0293 - val_acc: 0.9949\nEpoch 27/50\n1203/1203 [==============================] - 1s 584us/step - loss: 0.0125 - acc: 1.0000 - val_loss: 0.0252 - val_acc: 0.9949\nEpoch 28/50\n1203/1203 [==============================] - 1s 653us/step - loss: 0.0178 - acc: 0.9992 - val_loss: 0.0276 - val_acc: 0.9916\nEpoch 29/50\n1203/1203 [==============================] - 1s 581us/step - loss: 0.0158 - acc: 0.9967 - val_loss: 0.0295 - val_acc: 0.9916\nEpoch 30/50\n1203/1203 [==============================] - 1s 577us/step - loss: 0.0154 - acc: 0.9992 - val_loss: 0.0303 - val_acc: 0.9916\nEpoch 31/50\n1203/1203 [==============================] - 1s 577us/step - loss: 0.0146 - acc: 0.9983 - val_loss: 0.0280 - val_acc: 0.9949\nEpoch 32/50\n1203/1203 [==============================] - 1s 576us/step - loss: 0.0124 - acc: 0.9983 - val_loss: 0.0275 - val_acc: 0.9949\nEpoch 33/50\n1203/1203 [==============================] - 1s 512us/step - loss: 0.0136 - acc: 0.9975 - val_loss: 0.0307 - val_acc: 0.9949\nEpoch 34/50\n1203/1203 [==============================] - 1s 579us/step - loss: 0.0171 - acc: 0.9967 - val_loss: 0.0286 - val_acc: 0.9899\nEpoch 35/50\n1203/1203 [==============================] - 1s 580us/step - loss: 0.0110 - acc: 0.9992 - val_loss: 0.0272 - val_acc: 0.9933\nEpoch 36/50\n1203/1203 [==============================] - 1s 572us/step - loss: 0.0106 - acc: 0.9983 - val_loss: 0.0293 - val_acc: 0.9933\nEpoch 37/50\n1203/1203 [==============================] - 1s 579us/step - loss: 0.0116 - acc: 0.9975 - val_loss: 0.0264 - val_acc: 0.9933\nEpoch 38/50\n1203/1203 [==============================] - 1s 583us/step - loss: 0.0093 - acc: 0.9983 - val_loss: 0.0235 - val_acc: 0.9933\nEpoch 39/50\n1203/1203 [==============================] - 1s 592us/step - loss: 0.0122 - acc: 0.9975 - val_loss: 0.0233 - val_acc: 0.9966\nEpoch 40/50\n1203/1203 [==============================] - 1s 651us/step - loss: 0.0109 - acc: 0.9983 - val_loss: 0.0235 - val_acc: 0.9966\nEpoch 41/50\n1203/1203 [==============================] - 1s 589us/step - loss: 0.0109 - acc: 0.9983 - val_loss: 0.0268 - val_acc: 0.9933\nEpoch 42/50\n1203/1203 [==============================] - 1s 1ms/step - loss: 0.0103 - acc: 0.9967 - val_loss: 0.0217 - val_acc: 0.9949\nEpoch 43/50\n1203/1203 [==============================] - 1s 656us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.0217 - val_acc: 0.9949\nEpoch 44/50\n1203/1203 [==============================] - 1s 581us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.0266 - val_acc: 0.9949\nEpoch 45/50\n1203/1203 [==============================] - 1s 585us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.0249 - val_acc: 0.9966\nEpoch 46/50\n1203/1203 [==============================] - 1s 580us/step - loss: 0.0071 - acc: 0.9992 - val_loss: 0.0215 - val_acc: 0.9949\nEpoch 47/50\n1203/1203 [==============================] - 1s 582us/step - loss: 0.0088 - acc: 0.9992 - val_loss: 0.0277 - val_acc: 0.9916\nEpoch 48/50\n1203/1203 [==============================] - 1s 581us/step - loss: 0.0090 - acc: 0.9975 - val_loss: 0.0235 - val_acc: 0.9933\nEpoch 49/50\n1203/1203 [==============================] - 1s 580us/step - loss: 0.0086 - acc: 0.9992 - val_loss: 0.0254 - val_acc: 0.9916\nEpoch 50/50\n1203/1203 [==============================] - 1s 585us/step - loss: 0.0101 - acc: 0.9992 - val_loss: 0.0248 - val_acc: 0.9949\n"}, {"output_type": "execute_result", "data": {"text/plain": "<keras.callbacks.History at 0x7f37fc085b70>"}, "execution_count": 38, "metadata": {}}], "execution_count": 38}, {"source": "from keras import datasets", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 39}, {"source": "data=datasets.mnist.load_data()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n11493376/11490434 [==============================] - 11s 1us/step\n"}, {"output_type": "execute_result", "data": {"text/plain": "((array([[[0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          ..., \n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0]],\n  \n         [[0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          ..., \n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0]],\n  \n         [[0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          ..., \n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0]],\n  \n         ..., \n         [[0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          ..., \n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0]],\n  \n         [[0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          ..., \n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0]],\n  \n         [[0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          ..., \n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n  array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)),\n (array([[[0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          ..., \n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0]],\n  \n         [[0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          ..., \n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0]],\n  \n         [[0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          ..., \n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0]],\n  \n         ..., \n         [[0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          ..., \n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0]],\n  \n         [[0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          ..., \n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0]],\n  \n         [[0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          ..., \n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0],\n          [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n  array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)))"}, "execution_count": 40, "metadata": {}}], "execution_count": 40}, {"source": "## Store ", "cell_type": "markdown", "metadata": {}}, {"source": "fn = 'cnn.h5'", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 41}, {"source": "cnn.save(fn)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 42}, {"source": "!ls -al", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "total 1028\r\ndrwxr-x--- 2 dsxuser dsxuser   4096 Jan 21 10:03 .\r\ndrwx------ 1 dsxuser dsxuser   4096 Jan 21 09:16 ..\r\n-rw-r----- 1 dsxuser dsxuser 557784 Jan 21 10:03 cnn.h5\r\n-rw-r----- 1 dsxuser dsxuser 481767 Jan 21 10:03 cnn.tgz\r\n"}], "execution_count": 46}, {"source": "import os", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 44}, {"source": "os.system('tar cvfz cnn.tgz cnn.h5')", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "0"}, "execution_count": 45, "metadata": {}}], "execution_count": 45}, {"source": "", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3.5", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.5", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}